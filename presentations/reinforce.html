<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Fast Data</title>

    <meta name="description" content="Fast Data">
    <meta name="author" content="Bas Geerdink">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/bas.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

</head>

<!-- Projector—1280×720 native resolution, HDMI connections, Screen—16:9 aspect ratio-->

<body>
<div class="reveal">

    <!-- Any section element inside of this container is displayed as a slide -->
    <div class="slides">
        <section>
            <h1>FAST DATA</h1>
            <h3>Concepts, Architecture, Technology</h3>
            <p>
                <small>Bas Geerdink | October 6, 2023 | ReInforce Conference </small>
            </p>
            <img src="img/reinforce.png" style="height: 150px;" />
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <img src="img/Aizonic.png" style="height: 150px;" />
            <aside class="notes">
                Streaming analytics (or fast data) is becoming an increasingly popular subject in enterprise organizations because customers want to have real-time experiences, such as notifications and advice based on their online behavior and other users’ actions. A typical streaming analytics solution follows a “pipes and filters” pattern that consists of three main steps: detecting patterns on raw event data (complex event processing), evaluating the outcomes with the aid of business rules and machine learning algorithms, and deciding on the next action.

                Bas Geerdink details an open source reference solution for streaming analytics that covers many use cases that follow this pattern: actionable insights, fraud detection, log parsing, traffic analysis, factory data, the IoT, and others. The solution is built with the KFC stack: Kafka, Flink, and Cassandra. All source code is written in Scala.

                Bas explores a few architecture challenges that arise when dealing with streaming data, such as latency issues, event time versus server time, and exactly once processing. He provides architectural diagrams, explanations, a demo, and the source code. The solution (“Styx”) is open source and available on GitHub.
            </aside>
        </section>

        <section>
            <h2>Who am I?</h2>
            <pre><code data-trim>
                {
                  "name": "Bas Geerdink",
                  "role": "Technology Lead",
                  "company": ["Aizonic"],
                  "background": ["Artificial Intelligence",
                                 "Informatics"],
                  "mixins": ["Software engineering",
                             "Architecture",
                             "Innovation"],
                  "twitter": "@bgeerdink",
                  "linked_in": "bgeerdink"
                }
                </code></pre>
            <img src="img/Aizonic.png" style="width:150px;">
        </section>

        <section>
            <h2>Big Data</h2>
            <ul>
                <li>Volume</li>
                <li>Variety</li>
                <li>Velocity</li>
            </ul>
        </section>

        <section>
            <h2>Fast Data Use Cases</h2>
            <small>
                <table>
                    <thead>
                    <tr>
                        <th>Sector</th>
                        <th>Data source</th>
                        <th>Pattern</th>
                        <th>Notification</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>Retail</td>
                        <td>Clickstream</td>
                        <td>Product interest</td>
                        <td>Recommendation</td>
                    </tr>
                    <tr>
                        <td>Finance</td>
                        <td>Payment data</td>
                        <td>Fraud detection</td>
                        <td>Block money transfer</td>
                    </tr>
                    <tr>
                        <td>Insurance</td>
                        <td>Page visits</td>
                        <td>Trend analysis</td>
                        <td>Actionable insight</td>
                    </tr>
                    <tr>
                        <td>Healthcare</td>
                        <td>Patient data</td>
                        <td>Heart failure</td>
                        <td>Alert doctor</td>
                    </tr>
                    <tr>
                        <td>Traffic</td>
                        <td>Cars passing</td>
                        <td>Traffic jam</td>
                        <td>Update route info</td>
                    </tr>
                    <tr>
                        <td>Internet of Things</td>
                        <td>Machine logs</td>
                        <td>System failure</td>
                        <td>Alert to sys admin</td>
                    </tr>
                    </tbody>
                </table>
            </small>
            <aside class="notes">
                Mention: IoT, increase of data, importance of analytics, relation with big data.<br> Definition: streaming
                is a type of data processing engine that is designed with infinite data sets in mind (Tyler Akidau).<br>                    Real-time
                doesn't really exist; so, rather call it <i>fast data</i> processing.
            </aside>
        </section>

        <section>
            <h2>Fast Data Pattern</h2>
            <p align="left">There is a common pattern in all these scenarios:</p>
            <ol>
                <li>Prepare raw data:</li>
                <ul>
                    <li>Process each event <i>separately</i>; or</li>
                    <li>Detect pattern by <i>combining</i> events (CEP)</li>
                </ul>
                <li>Transform the data:</li>
                <ul>
                    <li>Process each event / combination; and/or    </li>
                    <li>Determine relevancy (ML)</li>
                </ul>
                <li>Produce follow-up action</li>
            </ol>
            <aside class="notes">
                One event doesn't mean much, only in combination with other event it gets useful.<br> Also: a lot of
                events can be ignored! This is a different pattern than <i>messages</i> in microservices, where each
                message has to be handled by (at least one) consumer.<br>
                CEP: Complete Event Processing, same as DSP: Digital Signal Processing <br> ML + feedback
                loop: AI (self-learning and adapting)
            </aside>
        </section>

        <section>
            <h2>Architecture</h2>
            <img src="img/architecture_spark_no_tech.png" class="stretch">
        </section>

        <section>
            <h2>A typical software stack</h2>
            <ul>
                <li>Data stream storage: <b>Kafka</b></li>
                <li>Cache: <b>Aerospike</b> or <b>Cassandra</b></li>
                <li>Stream processing: <b>Spark</b> or <b>Flink</b></li>
                <li>Model scoring: <b>ONNX</b> or <b>PMML</b></li>
            </ul>
            <aside class="notes">
                Considerations: latency, throughput, connectivity/integration.<br>
                All: scalable, mature, fault-tolerant, fast.<br>
                Spark has caught up with Flink in terms of performance, features, users.
            </aside>
        </section>

        <section>
            <h2>Architecture</h2>
            <img src="img/confluent.png" class="stretch">
        </section>

        <section>
            <h2>Architecture</h2>
            <img src="img/architecture_spark_flink.png" class="stretch">
        </section>

        <section>
            <h2>Fast Data applications</h2>
            <ul>
                <li>A Fast Data application is a running <i>job</i> that processes events in a data store (Kafka)</li>
                <li>Jobs can be deployed as ever-running pieces of software in a big data cluster (Spark, Flink, ...)</li>
                <li class="fragment" data-fragment-index="1">The basic pattern of a job is:
                    <ul class="fragment" data-fragment-index="1">
                        <li>Connect to the stream and <i>consume</i> events)</li>
                        <li>If needed, group the events (<i>windowing</i>)</li>
                        <li>Process each event or window (<i>aggregation</i>)</li>
                        <li>Write the results to a data store or stream (<i>sink</i>)</li>
                    </ul>
                </li>
            </ul>
        </section>

        <section>
            <h2>Parallelism</h2>
            <ul>
                <li>To get high throughput, we have to process the events in parallel</li>
                <li>Parallelism can be configured on <i>cluster</i> level and on <i>job</i> level</li>
                <li class="fragment" data-fragment-index="1">On cluster level: set the number of task slots (servers)
                    per job</li>
                <li class="fragment" data-fragment-index="1">On job level: distribute the data by grouping on a certain
                    data field</li>
            </ul>
        </section>

        <section>
            <h2>Event time</h2>
            <ul>
                <li>Events occur at certain time <span class="fragment fade-in" data-fragment-index="1">
                            <b>&rAarr; event time</b></span></li>
                <li>... but arrive a little while later <span class="fragment fade-in" data-fragment-index="1">
                            <b>&rAarr; ingestion time</b></span></li>
                <li>... and are processed later <span class="fragment fade-in" data-fragment-index="1">
                            <b>&rAarr; processing time</b></span></li>
            </ul>
            <img src="img/EventTime.png" class="fragment" style="max-width: 80%;" data-fragment-index="2" />

            <aside class="notes">
                Events can reach us <i>out-of-order</i>;
                by looking at the event time we can still detect the original order
            </aside>
        </section>

        <section>
            <h2>Out-of-orderness</h2>
            <img src="img/EventTime_StarWars.png" class="stretch" /><br>
            <img src="img/star-wars.png" style="width:200px;" />
        </section>

        <section>
            <h2>Windows</h2>
            <ul>
                <li>In processing infinite streams, we usually look at a time <i>window</i> </li>
                <li>A windows can be considered as a <i>bucket of time</i></li>
                <li class="fragment" data-fragment-index="1">There are different types of windows:
                    <ul class="fragment" data-fragment-index="1">
                        <li>Sliding window</li> <!-- e.g. looking back at the last hour, continuous updating average of system activity -->
                        <li>Tumbling window</li> <!-- e.g. processing data per day to get a financial report -->
                        <li>Session window</li> <!-- based on event data, e.g. users logged in and out. Not in Spark! -->
                    </ul>
                </li>
                <aside class="notes">
                    Windows can be very large (e.g. weeks) and thereby gather a lot of state. At the end of a window, a function (Window operator)
                    is called.
                    In Spark: a window evaluation is an _action_ (as opposed to transformation).
                </aside>
            </ul>
        </section>

        <section>
            <h2>Windows</h2>
            <img src="img/Windows.png" class="stretch" />
        </section>

        <section>
            <h2>Window considerations</h2>
            <ul>
                <li><b>Size</b>: large windows lead to big state and long calculations</li>
                <li><b>Number</b>: many windows (e.g. sliding, session) lead to more calculations</li>
                <li><b>Evaluation</b>: do all calculations within one window, or keep a cache across multiple windows (e.g. when comparing windows, like in trend analysis)</li>
                <li><b>Timing</b>: events for a window can appear <i>early</i> or <i>late</i></li>
            </ul>
            <aside class="notes">
                <p>Fraud detection: sliding window of 1 day</p>
                <p>Actionable insights: tumbling window of 1 hour</p>
            </aside>
        </section>

        <section>
            <h2>Watermarks</h2>
            <ul>
                <li><i>Watermarks</i> are timestamps that trigger the computation of the window</li>
                <li>They are generated at a time that allows a bit of slack for <i>late events</i></li>
                <li class="fragment" data-fragment-index="1">Any event that reaches the processor later
                    than the watermark, but with an event time that should belong to the former window, is <i>ignored</i></li>
                <li class="fragment" data-fragment-index="2">In Flink, it's possible to allow late events to trigger re-computation
                    of a window by setting the <i>allowedLateness</i> property</li>
            </ul>
        </section>

        <section>
            <h2>Consistency semantics</h2>
            <ul>
                <li>By configuring offsets, state, and savepointing/checkpointing, a stream processing system can reach a level of robustness:</li>
                <ul>
                    <li><b>At-least-once</b>: guarantees that each event is always processed</li>
                    <li><b>At-most-once</b>: guarantees that an event is never processed more than once</li>
                    <li><b>Exactly-once</b>: guarantees that an event is always processed once</li>
                </ul>
            </ul>
        </section>

        <section>
            <h2>Exactly-once processing</h2>
            <ul>
                <li>An event has three possible statuses:</li>
                <ul>
                    <li>Not processed (in cache on the message bus)</li>
                    <li>In transit (picked up by a job, in a window) = <i>state</i></li>
                    <li>Processed (handled by the job)</li>
                </ul>
                <li class="fragment" data-fragment-index="1">Kafka knows for each consumer which data has been read:
                    <i>offset</i></li>
                <li class="fragment" data-fragment-index="1">Flink has <i>checkpoints</i> that allow to replay the
                    stream in case of failures</li>
                <li class="fragment" data-fragment-index="1">This combination guarantees that an event goes through
                    the system <i>exactly once</i></li>
            </ul>
            <aside class="notes">
                Other options for semantics are <b>at-least-once</b> and <b>at-most-once</b> processing
            </aside>
        </section>

        <section>
            <h2>Savepointing and checkpointing</h2>
            <ul>
                <li>A <i>checkpoint</i> is a periodic dump to a file on disc of the in-memory state</li>
            </ul>
            <pre><code data-trim>
   env.enableCheckPointing(10000)  // checkpoint every 10 seconds
                </code></pre>
            <ul>
                <li>A <i>savepoint</i> is a manual checkpoint</li>
                <li>The state dumps can be used for recovery and replay</li>
            </ul>
            <pre><code data-trim>
    # Supported backends: jobmanager, filesystem, rocksdb
    #
    state.backend: filesystem
                </code></pre>
            <aside class="notes">
                Trade-off: size of state vs speed of recovery/updates
            </aside>
        </section>

        </section>

        <section>
            <h2>Model scoring</h2>
            <ul>
                <li>To determine the follow-up action of a aggregated business event (e.g. pattern), we have to enrich the event with customer data</li>
                <li>The resulting data object contains the characteristics (features) as input for a model</li>
                <!--                    <li class="fragment" data-fragment-index="1">To get the features and score the model, efficiency plays a role again:</li>-->
                <!--                    <ul class="fragment" data-fragment-index="1">-->
                <!--                        <li>Direct database call > API call</li>-->
                <!--                        <li>In-memory model cache > model on disk</li>-->
                <!--                    </ul>-->
            </ul>
        </section>

        <section>
            <h2>Cross-platform model formats</h2>
            <ul>
                <li>Data scientists can <i>export</i> their machine learning models from Python</li>
                <li>Popular formats are: ONNX, PMML, and PFA</li>
            </ul>
            <pre><code data-trim>
from sklearn.linear_model import LogisticRegression
from sklearn2pmml import sklearn2pmml

events_df = pandas.read_csv("events.csv")

pipeline = PMMLPipeline(...)
pipeline.fit(events_df, events_df["notifications"])

sklearn2pmml(pipeline, "LogisticRegression.pmml", with_repr = True)
            </code></pre>
        </section>

        <section>
            <h2>PMML</h2>
            <img src="img/PMML.png" class="stretch" />
        </section>

        <!--            <section>-->
        <!--                <h2>Model scoring</h2>-->
        <!--                <ul>-->
        <!--                    <li>The models can be loaded into memory to enable split-second performance</li>-->
        <!--                    <li>By applying <i>map</i> functions over the events we can process/transform the-->
        <!--                        data in the windows:</li>-->
        <!--                        <ol>-->
        <!--                            <li>enrich each business event by getting more data</li>-->
        <!--                            <li>filtering events based on selection criteria (rules)</li>-->
        <!--                            <li>score a machine learning model on each event</li>-->
        <!--                            <li>write the outcome to a new event / output stream</li>-->
        <!--                        </ol>-->
        <!--                    </li>-->
        <!--                </ul>-->
        <!--            </section>-->

        <section>
            <h2>Alternative stacks</h2>
            <small>
                <table>
                    <thead>
                    <tr>
                        <th>Message bus</th>
                        <th>Streaming technology</th>
                        <th>Database</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>Kafka</td>
                        <td>Spark Structured Streaming</td>
                        <td>Ignite</td>
                    </tr>
                    <tr>
                        <td>Kafka</td>
                        <td>Flink</td>
                        <td>Cassandra</td>
                    </tr>
                    <tr>
                        <td>Azure Event Hubs</td>
                        <td>Azure Stream Analytics</td>
                        <td>Cosmos DB</td>
                    </tr>
                    <tr>
                        <td>AWS Kinesis Data Streams</td>
                        <td>AWS Kinesis Data Firehose</td>
                        <td>DynamoDb</td>
                    </tr>

                    </tbody>
                </table>
            </small>
        </section>

        <section>
            <h2>Wrap-up</h2>
            <ul>
                <li>Stream processing is a form of big data that is becoming increasingly popular</li>
                <li>The common pattern is: CEP &rarr; ML &rarr; Notification</li>
                <li>Pick the right tools for the job; Kafka, Flink, and Cassandra are amongst the best</li>
                <li>Be aware of typical <i>fast data</i> issues: late events, state management, windows, etc.</li>
            </ul>
        </section>

        <section>
            <h2>Thanks!</h2>
            <small>
                <p>Read more about streaming analytics at:</p>
                <ul>
                    <li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf">Google
                        Dataflow paper</a></li>
                    <li><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">The world beyond batch:
                        Streaming 101</a></li>
                    <li><a href="https://online-learning.tudelft.nl/courses/taming-big-data-streams-real-time-data-processing-at-scale/">Real-time Data Processing at Scale (TU Delft)</a></li>
                </ul>
                <p>Source code and presentation are available on Github:</p>
                <p>
                    <a href="https://github.com/streaming-analytics/Styx">https://github.com/streaming-analytics/Styx</a>
                </p>
                <p>Please connect at <a href="https://aizonic.com">Aizonic.com</a>, <a href="https://nl.linkedin.com/in/geerdink">LinkedIn</a> or <a href="https://twitter.com/bgeerdink">Twitter</a></p>
            </small>
            <a href="https://aizonic.com"><img src="img/Aizonic.png" style="width: 200px;"></a>
        </section>

        <!--            <div id="Aizonic-logo" style="position: absolute; bottom: 0px; left: 10px; width: 50px; height: 50px;">-->
        <!--                <img src="img/Aizonic.png">-->
        <!--            </div>-->

        <!--            <div id="footer">-->
        <!--                <img src="img/strata_footer.png" class="stretch" style="width:100%; min-width:100%;">-->
        <!--            </div>-->

    </div>

</div> <!-- reveal -->

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
        // More info https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            transition: 'slide', // none/fade/slide/convex/concave/zoom

            // More info https://github.com/hakimel/reveal.js#dependencies
            dependencies: [
                { src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
                { src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: 'plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
                { src: 'plugin/search/search.js', async: true },
                { src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/notes/notes.js', async: true }
            ]
        });

    </script>

</body>

</html>
