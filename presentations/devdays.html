<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Styx</title>

	<meta name="description" content="Styx Streaming Analytics">
	<meta name="author" content="Bas Geerdink">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/bas.css" id="theme">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	<!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

	<!-- Any section element inside of this container is displayed as a slide -->
	<div class="slides">
		<section>
			<h1>Streaming Analytics</h1>
            <h3>How to get fast predictions from real-time data</h3>
            <h3>with Flink, Kafka, Cassandra and PMML</h3>
			<p>
				<small>Bas Geerdink | DevDays Riga | 31 May 2018</small>
			</p>
		</section>

		<section>
			<h2>Agenda</h2>
			<ol>
                <li>Streaming Analytics use cases</li>
                <li>Architecture and Technology</li>
                <li>Deep dive:
                    <ul>
                        <li>Kafka-Flink connection</li>
                        <li>Parallel processing</li>
                        <li>Event Time, Windows, and Watermarks</li>
                        <li>Exactly-once Processing</li>
                        <li>Model scoring</li>
                    </ul>
                </li>
                <li>Wrap-up</li>
			</ol>
		</section>

        <section>
            <h2>Use Cases</h2>
            <small>
                <table>
                    <thead><tr>
                        <th>Sector</th>
                        <th>Data source</th>
                        <th>Pattern + prediction</th>
                        <th>Notification</th>
                    </tr></thead>
                    <tbody>
                        <tr>
                            <td>Finance</td>
                            <td>Debit card transactions</td>
                            <td>Running out of credit</td>
                            <td>Actionable insight</td>
                        </tr>
                        <tr>
                            <td>Finance</td>
                            <td>Payment data</td>
                            <td>Fraud detection</td>
                            <td>Block money transfer</td>
                        </tr>
                        <tr>
                            <td>Insurance</td>
                            <td>Page visits</td>
                            <td>Customer is stuck in form</td>
                            <td>Chat window</td>
                        </tr>
                        <tr>
                            <td>Healthcare</td>
                            <td>Patient data</td>
                            <td>Heart failure</td>
                            <td>Alert doctor</td>
                        </tr>
                        <tr>
                            <td>Retail</td>
                            <td>Tweets</td>
                            <td>Sentiment analysis</td>
                            <td>Social media response</td>
                        </tr>
                        <tr>
                            <td>Traffic</td>
                            <td>Number of cars passing</td>
                            <td>Traffic jam</td>
                            <td>Update route info</td>
                        </tr>
                        <tr>
                            <td>Manufacturing</td>
                            <td>Logs</td>
                            <td>System failure</td>
                            <td>Alert to sys admin</td>
                        </tr>
                    </tbody>
                </table>
            </small>
            <aside class="notes">
                Mention: IoT, increase of data, importance of analytics, relation with big data.<br>
                Definition: streaming is a type of data processing engine that is designed with infinite data sets in mind (Tyler Akidau).<br>
                Real-time doesn't really exist; so, rather call it <i>fast data</i> processing.
            </aside>
        </section>

        <section>
            <h2>Streaming Analytics Pattern</h2>
            The common pattern in all these scenarios:
            <ol>
                <li>Detect pattern by combining data (CEP)</li>
                <li>Determine relevancy (ML)</li>
                <li>Produce notification</li>
            </ol>
            <aside class="notes">
                One event doesn't mean much, only in combination with other event it gets useful.<br>
                Also: a lot of events can be ignored!
                This is a different pattern than <i>messages</i> in microservices, where each message has to be handled by (at least one) consumer.<br>
                CEP: Digital Signal Processing <br>
                ML + feedback loop: AI (self-learning and adapting)
            </aside>
        </section>

		<section>
            <h2>Architecture</h2>
            <img src="img/architecture.png" class="stretch">
        </section>

        <section>
            <h2>Technology</h2>
            Styx uses a selection of source frameworks
            <ul>
                <li>Data stream storage: <b>Kafka</b></li>
                <li>Stream processing: <b>Flink</b></li>
                <li>Persisting rules, models, and config: <b>Cassandra</b></li>
                <li>Model scoring: <b>PMML</b> and <b>Openscoring.io</b></li>
            </ul>
            <aside class="notes">
                Considerations: latency, throughput, connectivity/integration.<br>
                All: scalable, mature, fault-tolerant, fast.
            </aside>
        </section>

        <section>
            <section>
                <h2>Deep dive part 1</h2>
                <img src="img/part1.png" class="stretch">
            </section>

            <section>
                <h2>Set up a Flink application</h2>
                <ul>
                    <li>A Flink job is a Java/Scala application</li>
                    <li>Jobs (.jar) can be deployed to a running Flink cluster</li>
                    <li>Maven dependencies: </li>
                </ul>
                <pre><code data-trim>
                    <dependency>
                      <groupId>org.apache.flink</groupId>
                      <artifactId>flink-scala_2.11</artifactId>
                      <version>1.5.0</version>
                      <scope>provided</scope>
                    </dependency>
                    <dependency>
                      <groupId>org.apache.flink</groupId>
                      <artifactId>flink-streaming-scala_2.11</artifactId>
                      <version>1.5.0</version>
                      <scope>provided</scope>
                    </dependency>
                </code></pre>
            </section>

            <section>
                <h2>Flink-Kafka integration</h2>
                The <i>FlinkKafkaConsumer</i> and <i>FlinkKafkaProducer</i> classes provide easy connectivity
                <!-- Simplified code: serializing data and making a DataStream of 'raw' Kafka events -->
                <pre><code data-trim>
def createKafkaEventStream(env: StreamExecutionEnvironment):
      DataStream[RawEvent] = {

    // create KeyedDeserializationSchema
    val readSchema = KafkaSchemaFactory.createKeyedDeserializer
            (readTopicDef, rawEventFromPayload)

    // consume events
    val rawEventSource = new FlinkKafkaConsumer010[BaseKafkaEvent]
            (toStringList(readTopicDef), readSchema, props)

    // create source
    env.addSource(rawEventSource)
      .filter(_.isSuccess)   // if decrypting from kafka succeeded
      .flatMap(_.toOption)
}
                </code></pre>
            </section>

            <section>
                <h2>Parallelism</h2>
                <ul>
                    <li>To get high throughput, we have to process the events in parallel</li>
                </ul>
                <pre><code data-trim>
                    taskmanager.numberOfTaskSlots: 4
                    parallelism.default: 2
                </code></pre>
                <ul>
                    <li>The <i>keyBy</i> function distributes our stream into parallel parts</li>
                </ul>
                <pre><code data-trim>
                      def createCepPipeline(sourceStream: DataStream[RawEvent]):
                            DataStream[BusinessEvent] = {
                        val windowStream = sourceStream
                          .keyBy(_.customerId.head)           // create parallelism: 0..9
                          .window(GlobalWindows.create())     // window per key
                          .apply(new WindowResultFunction())  // logic: pattern match
                          .map(event => event.addTimeStamp("window"))
                      }
                </code></pre>
            </section>

            <section>
                <h2>Event time</h2>
                <ul>
                    <li>Events occur at certain time <span class="fragment fade-in" data-fragment-index="1">
                        <b>&rAarr; event time</b></span></li>
                    <li>... but arrive a little while later <span class="fragment fade-in" data-fragment-index="1">
                        <b>&rAarr; processing time</b></span></li>
                </ul>
                <img src="img/EventTime.png" class="fragment" data-fragment-index="2"/>
                <pre class="fragment" data-fragment-index="3"><code data-trim>
                    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
                </code></pre>
                <aside class="notes">
                    Events can reach us <i>out-of-order</i>; by looking at the event time we can still detect the original order</li>
                </aside>
            </section>

            <section>
                <h2>Out-of-orderness</h2>
                <img src="img/EventTime_StarWars.png" class="stretch"/><br>
                <img src="img/Star_Wars_logo.png" style="width:200px;"/>
            </section>

            <section>
                <h2>Windows</h2>
                <ul>
                    <li>In processing infinite streams, we usually look at a time <i>window</i> </li>
                    <li>A windows can be considered as a <i>bucket of time</i></li>
                    <li class="fragment" data-fragment-index="1">There are different types of windows:
                        <ul class="fragment" data-fragment-index="1">
                            <li>Sliding window</li> <!-- e.g. looking back at the last hour, continuous updating average of system activity -->
                            <li>Tumbling window</li> <!-- e.g. processing data per day to get a financial report -->
                            <li>Session window</li> <!-- based on event data, e.g. users logged in and out -->
                            <!-- <li>Global window</li> --> <!-- per key, e.g. check _all_ user activity -->
                        </ul>
                    </li>
                    <aside class="notes">
                         Windows can be very large (e.g. weeks) and thereby gather a lot of state.
                         At the end of a window, a function (Window operator) is called.
                    </aside>
                </ul>
            </section>

            <section>
                <h2>Windows</h2>
                <img src="img/Windows.png" class="stretch" />
            </section>


            <section>
                <h2>Watermarks</h2>
                <ul>
                    <li><i>Watermarks</i> are timestamps that trigger the computation of the window</li>
                    <li>They are generated at a time that allows a bit of slack for <i>late events</i></li>
                    <li>By default, any event that reaches the processor later than the watermark, but with an event time that should belong to the previous window, is ignored</li>
                    <li>It's possible to allow late events to trigger recomputation of a window by setting the <i>allowedLateness</i> property</li>
                </ul>
                <aside class="notes">
                    How do we know when to calculate a window?<br>
                    Watermarks assert that all earlier events have (probably) arrived<br>
                    (other triggers are also possible: processing time, punctuations (data-dependent).<br>
                    IF eventTime < watermark THEN Ignore ELSE Process
                </aside>
            </section>

            <section>
                <h2>Watermarks</h2>
                <ul>
                    <li>In Flink, we inject a <i>Timestamp Assigner & Watermark Generator</i></li>
                </ul>
                <pre><code data-trim>
                    createKafkaEventStream(env, readTopicDef, kafkaProperties, jobName)
                        .assignTimestampsAndWatermarks(
                             new TimedEventWatermarkExtractor[T]())
                </code></pre>
                <ul>
                    <li>There are <i>periodic</i> (timer-based) and <i>punctuated</i> (stream-based) watermarks:</li>
                    <ul>
                        <li>AssignerWithPeriodicWatermarks</li> <!-- assigns timestamps and generates watermarks periodically -->
                        <li>AssignerWithPunctuatedWatermarks</li> <!-- generate watermarks whenever a certain event indicates that a new watermark might be generated -->
                    </ul>
                </ul>
                <aside class="notes">
                    Example: tumbling window of one hour, watermark is 1 minute. af 14:02 an event comes in with an event time of 13:59
                </aside>
            </section>

            <section>
                <h2>Event Time and Watermarks</h2>
                <pre><code data-trim>
                    class TimedEventWatermarkExtractor
                       extends AssignerWithPeriodicWatermarks[TimedEvent]() {

                        // specify the event time
                        override def extractTimestamp(element: TimedEvent,
                                        previousElementTimestamp: Long): Long = {
                            // set event time to 'eventTime' field in events
                            element.eventTime.getMillis
                        }

                        // this method is called to emit a watermark every time the
                        // ExecutionConfig.setAutoWatermarkInterval(...) interval occurs
                        override def getCurrentWatermark: Watermark = {
                            // one second delay for processing of window
                            new Watermark(System.currentTimeMillis() - 1000)
                      }
                    }
                </code></pre>
            </section>

            <section>
                <h2>Exactly-once processing</h2>
                <ul>
                    <li>An event has three possible statuses:</li>
                    <ul>
                        <li>Not processed (in cache on the Kafka bus)</li>
                        <li>In transit (picked up by Flink, in a window) = <i>state</i></li>
                        <li>Processed (handled by Flink)</li>
                    </ul>
                    <li class="fragment" data-fragment-index="1">Kafka knows for each consumer which data has been read: <i>offset</i></li>
                    <li class="fragment" data-fragment-index="1">Flink has <i>checkpoints</i> that allow to replay the stream in case of failures</li>
                    <li class="fragment" data-fragment-index="1">This combination guarantees that an event goes through the system <i>exactly once</i></li>
                </ul>
                <aside class="notes">
                    Other options for semantics are <b>at-least-once</b> and <b>at-most-once</b> processing
                </aside>
            </section>

            <section>
                <h2>Savepointing and checkpointing</h2>
                <ul>
                    <li>A <i>checkpoint</i> is a periodic dump to file of the in-memory state</li>
                </ul>
                <pre><code data-trim>
   env.enableCheckPointing(10000)  // checkpoint every 10 seconds
                </code></pre>
                <ul>
                    <li>A <i>savepoint</i> is a manual checkpoint</li>
                    <li>The state dumps can be used for recovery and replay</li>
                </ul>
                <pre><code data-trim>
    # Supported backends: jobmanager, filesystem, rocksdb
    #
    state.backend: filesystem
                </code></pre>
                <aside class="notes">
                    Trade-off: size of state vs speed of recovery/updates
                </aside>
            </section>

            <section>
                <h2>Upgrading a Flink job</h2>
                <pre><code data-trim>
                    # show the current list of jobs: gives job id
                    /flink/bin/flink list

                    # create a savepoint
                    /flink/bin/flink savepoint [JOB_ID]

                    # wait for response: gives savepoint id
                    /flink/bin/flink cancel [JOB_ID]

                    # start the new job
                    /flink/bin/flink run -p 4 \
                        -s jobmanager://savepoints/[SAVEPOINT_ID] \
                        -c com.styx.StyxAppJob styx-app-0.0.1.jar \
                        --config /opt/flink/jars/reference.conf
                </code></pre>
            </section>

            <section>
                <h2>Flink CEP</h2>
                <pre><code data-trim>
val cep = Pattern.begin[ClickEvent]("Page1").within(Time.seconds(1))
    .where( event =>
        event match {
          case c: ClickEvent => c.processStep.equals(steps(0))
          case _ => false
        })
    .next("Page2")  // check that Page1 is followed by Page2
      .where(event => event match {
          case c: ClickEvent => c.processStep.equals(steps(1))
          case _ => false
        })
    .next("Page3")  // check that Page2 is followed by Page3
    .where(event => event match {
        case c: ClickEvent => c.processStep.equals(steps(2))
        case _ => false
      })
    .within(Time.minutes(30))  // no match = pattern detected
  }
                </code></pre>
            </section>

        </section>

        <section>
            <section>
                <h2>Deep dive part 2</h2>
                <img src="img/part2.png" class="stretch"/>
            </section>

            <section>
                <h2>PMML</h2>
                <ul>
                    <li>Data scientists who build and train a machine learning model can export it to PMML (or PFA) format</li>
                </ul>
                <pre><code data-trim>
from sklearn.linear_model import LogisticRegression
from sklearn2pmml import sklearn2pmml

events_df = pandas.read_csv("events.csv")

pipeline = PMMLPipeline(...)
pipeline.fit(events_df, events_df["notifications"])

sklearn2pmml(pipeline, "LogisticRegression.pmml", with_repr = True)
                </code></pre>
                <aside class="notes">
                    PMML is the glue between data science and data engineering.
                </aside>
            </section>

            <section>
                <h2>PMML</h2>
                <img src="img/PMML.png" class="stretch"/>
            </section>

            <section>
                <h2>Model scoring</h2>
                <ul>
                    <li>By doing a <i>flatMap</i> over the events we can apply functions that process/transform the data in the windows</li>
                    <li>For example:
                        <ol>
                            <li>enrich each business event by getting more data</li>
                            <li>filtering events based on selection criteria (rules)</li>
                            <li>score a machine learning model on each event</li>
                            <li>write the outcome to a new event / output stream</li>
                        </ol>
                    </li>
                </ul>
            </section>

            <section>
                <h2>Openscoring.io</h2>
                <pre><code data-trim>
                def score(event: RichBusinessEvent, pmmlModel: PmmlModel): Double = {
                    val arguments = new util.LinkedHashMap[FieldName, FieldValue]

                    for (inputField: InputField <- pmmlModel.getInputFields.asScala) {
                        arguments.put(inputField.getField.getName,
                            inputField.prepare(customer.all(fieldName.getValue)))
                    }

                    // return the notification with a relevancy score
                    val results = pmmlModel.evaluate(arguments)

                    pmmlModel.getTargetFields.asScala.headOption match {
                        case Some(targetField) =>
                            val targetFieldValue = results.get(targetField.getName)
                        case _ => throw new Exception("No valid target")
                        }
                    }
                }
                </code></pre>
            </section>

        </section>

        <section>
            <h2>Wrap-up</h2>
            <ul>
                <li>There are plenty of streaming analytics use cases, in any business domain</li>
                <li>The common pattern is: CEP &rarr; ML &rarr; Notification</li>
                <li>Pick the right tools for the job; Kafka and Flink are amongst the best</li>
                <li>Be aware of typical streaming data issues: late events, state management, distribution</li>
                <li>Styx is a proven open source framework for Streaming Analytics that is flexible and scalable</li>
            </ul>
        </section>

        <section>
            <h2>Thanks!</h2>
            <p>Read more about streaming analytics at:</p>
            <ul>
                <li><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">The world beyond batch: Streaming 101</a></li>
                <li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf">Google Dataflow paper</a></li>
            </ul>
            <p>Source code and presentation is available at:</p>
            <p>
                <a href="https://github.com/streaming-analytics/Styx">https://github.com/streaming-analytics/Styx</a>
            </p>
        </section>

	</div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/search/search.js', async: true },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

</script>

</body>
</html>
