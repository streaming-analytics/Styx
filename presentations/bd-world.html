<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Fast Data</title>

    <meta name="description" content="Fast Data">
    <meta name="author" content="Bas Geerdink">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/bas.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

</head>

<!-- Projector—1280×720 native resolution, HDMI connections, Screen—16:9 aspect ratio-->

<body>
    <div class="reveal">

        <!-- Any section element inside of this container is displayed as a slide -->
        <div class="slides">
            <section>
                <h2>Streaming Analytics</h2>
                <h3>Concepts, Architecture, Technology</h3>
                <img src="img/bdw.png" style="height: 250px;" />
                <p>
                    <small>Bas Geerdink | March 11, 2020 | Big Data & AI World </small>
                </p>
                <aside class="notes">
                        Streaming analytics (or fast data) is becoming an increasingly popular subject in enterprise organizations because customers want to have real-time experiences, such as notifications and advice based on their online behavior and other users’ actions. A typical streaming analytics solution follows a “pipes and filters” pattern that consists of three main steps: detecting patterns on raw event data (complex event processing), evaluating the outcomes with the aid of business rules and machine learning algorithms, and deciding on the next action.

                        Bas Geerdink details an open source reference solution for streaming analytics that covers many use cases that follow this pattern: actionable insights, fraud detection, log parsing, traffic analysis, factory data, the IoT, and others. The solution is built with the KFC stack: Kafka, Flink, and Cassandra. All source code is written in Scala.
                        
                        Bas explores a few architecture challenges that arise when dealing with streaming data, such as latency issues, event time versus server time, and exactly once processing. He provides architectural diagrams, explanations, a demo, and the source code. The solution (“Styx”) is open source and available on GitHub.
                </aside>
            </section>

            <section>
                <h2>Who am I?</h2>
                <pre><code data-trim>
                { 
                  "name": "Bas Geerdink",
                  "role": "Technology Lead",
                  "company": ["Aizonic"],
                  "background": ["Artificial Intelligence",
                                 "Informatics"],
                  "mixins": ["Software engineering",
                             "Architecture",
                             "Innovation"],
                  "twitter": "@bgeerdink",
                  "linked_in": "bgeerdink"
                }
                </code></pre>
                <img src="img/Aizonic.png" style="width:150px;">
            </section>

            <section>
                <h2>Big Data</h2>
                <ul>
                    <li>Volume</li>
                    <li>Variety</li>
                    <li>Velocity</li>
                </ul>
            </section>

            <section>
                <h2>Fast Data Use Cases</h2>
                <small>
                    <table>
                        <thead>
                            <tr>
                                <th>Sector</th>
                                <th>Data source</th>
                                <th>Pattern</th>
                                <th>Notification</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Retail</td>
                                <td>Clickstream</td>
                                <td>Product interest</td>
                                <td>Recommendation</td>
                            </tr>
                            <tr>
                                <td>Finance</td>
                                <td>Payment data</td>
                                <td>Fraud detection</td>
                                <td>Block money transfer</td>
                            </tr>
                            <tr>
                                <td>Insurance</td>
                                <td>Page visits</td>
                                <td>Trend analysis</td>
                                <td>Actionable insight</td>
                            </tr>
                            <tr>
                                <td>Healthcare</td>
                                <td>Patient data</td>
                                <td>Heart failure</td>
                                <td>Alert doctor</td>
                            </tr>
                            <tr>
                                <td>Traffic</td>
                                <td>Cars passing</td>
                                <td>Traffic jam</td>
                                <td>Update route info</td>
                            </tr>
                            <tr>
                                <td>Internet of Things</td>
                                <td>Machine logs</td>
                                <td>System failure</td>
                                <td>Alert to sys admin</td>
                            </tr>
                        </tbody>
                    </table>
                </small>
                <aside class="notes">
                    Mention: IoT, increase of data, importance of analytics, relation with big data.<br> Definition: streaming
                    is a type of data processing engine that is designed with infinite data sets in mind (Tyler Akidau).<br>                    Real-time
                    doesn't really exist; so, rather call it <i>fast data</i> processing.
                </aside>
            </section>

            <section>
                <h2>Fast Data Pattern</h2>
                <p align="left">There is a common pattern in all these scenarios:</p>
                <ol>
                    <li>Prepare raw data:</li>
                    <ul>
                        <li>Process each event <i>separately</i>; or</li>
                        <li>Detect pattern by <i>combining</i> events (CEP)</li>
                    </ul>
                    <li>Determine relevancy (ML)</li>
                    <li>Produce follow-up action</li>
                </ol>
                <aside class="notes">
                    One event doesn't mean much, only in combination with other event it gets useful.<br> Also: a lot of
                    events can be ignored! This is a different pattern than <i>messages</i> in microservices, where each
                    message has to be handled by (at least one) consumer.<br>
                    CEP: Complete Event Processing, same as DSP: Digital Signal Processing <br> ML + feedback
                    loop: AI (self-learning and adapting)
                </aside>
            </section>

            <section>
                <h2>Architecture</h2>
                <img src="img/architecture_spark_no_tech.png" class="stretch">
            </section>

            <section>
                <h2>A possible software stack</h2>
                <ul>
                    <li>Data stream storage: <b>Kafka</b></li>
                    <li>Cache: any distributed low-latency NoSQL database, e.g. <b>Cassandra</b></li>
                    <li>Stream processing: <b>Spark</b> or <b>Flink</b></li>
                    <li>Model scoring: <b>PMML</b> and <b>Openscoring.io</b></li>
                </ul>
                <aside class="notes">
                    Considerations: latency, throughput, connectivity/integration.<br>
                    All: scalable, mature, fault-tolerant, fast.<br>
                    Spark has caught up with Flink in terms of performance, features, users.
                </aside>
            </section>

            <section>
                <h2>Architecture</h2>
                <img src="img/architecture_spark_flink.png" class="stretch">
            </section>

            <section>
                <h2>Fast Data applications</h2>
                <ul>
                    <li>A Fast Data application is a running <i>job</i> that processes events in a data store (Kafka)</li>
                    <li>Jobs can be deployed as ever-running pieces of software in a big data cluster (Spark, Flink, ...)</li>
                    <li class="fragment" data-fragment-index="1">The basic pattern of a job is:
                        <ul class="fragment" data-fragment-index="1">
                            <li>Connect to the stream and consume events (<i>ingestion</i>)</li>
                            <li>If needed, group the events (<i>windowing</i>)</li>
                            <li>Process each event or window (<i>aggregation</i>)</li>
                            <li>Write the results to a data store or stream (<i>sink</i>)</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section>
                <h2>Parallelism</h2>
                <ul>
                    <li>To get high throughput, we have to process the events in parallel</li>
                    <li>Parallelism can be configured on <i>cluster</i> level and on <i>job</i> level (number of worker threads)</li>
                </ul>

                <pre><code data-trim>
val conf = new SparkConf()
.setMaster("local[8]")
.setAppName("FraudNumberOfTransactions")
                </code></pre>

                <pre><code data-trim>
./bin/spark-submit --name "LowMoneyAlert" --master local[4]
--conf "spark.dynamicAllocation.enabled=true"
--conf "spark.dynamicAllocation.maxExecutors=2" styx.jar
                </code></pre>
            </section>

            <section>
                <h2>Event time</h2>
                <ul>
                    <li>Events occur at certain time <span class="fragment fade-in" data-fragment-index="1">
                            <b>&rAarr; event time</b></span></li>
                    <li>... but arrive a little while later <span class="fragment fade-in" data-fragment-index="1">
                            <b>&rAarr; ingestion time</b></span></li>
                    <li>... and are processed later <span class="fragment fade-in" data-fragment-index="1">
                            <b>&rAarr; processing time</b></span></li>
                </ul>
                <img src="img/EventTime.png" class="fragment" style="max-width: 80%;" data-fragment-index="2" />

                <aside class="notes">
                    Events can reach us <i>out-of-order</i>;
                    by looking at the event time we can still detect the original order
                </aside>
            </section>

            <section>
                <h2>Out-of-orderness</h2>
                <img src="img/EventTime_StarWars.png" class="stretch" /><br>
                <img src="img/Star_Wars_logo.png" style="width:200px;" />
            </section>

            <section>
                <h2>Windows</h2>
                <ul>
                    <li>In processing infinite streams, we usually look at a time <i>window</i> </li>
                    <li>A windows can be considered as a <i>bucket of time</i></li>
                    <li class="fragment" data-fragment-index="1">There are different types of windows:
                        <ul class="fragment" data-fragment-index="1">
                            <li>Sliding window</li> <!-- e.g. looking back at the last hour, continuous updating average of system activity -->
                            <li>Tumbling window</li> <!-- e.g. processing data per day to get a financial report -->
                            <li>Session window</li> <!-- based on event data, e.g. users logged in and out. Not in Spark! -->
                        </ul>
                    </li>
                    <aside class="notes">
                        Windows can be very large (e.g. weeks) and thereby gather a lot of state. At the end of a window, a function (Window operator)
                        is called.
                        In Spark: a window evaluation is an _action_ (as opposed to transformation).
                    </aside>
                </ul>
            </section>

            <section>
                <h2>Windows</h2>
                <img src="img/Windows.png" class="stretch" />
            </section>

            <section>
                <h2>Window considerations</h2>
                <ul>
                    <li><b>Size</b>: large windows lead to big state and long calculations</li>
                    <li><b>Number</b>: many windows (e.g. sliding, session) lead to more calculations</li>
                    <li><b>Evaluation</b>: do all calculations within one window, or keep a cache across multiple windows (e.g. when comparing windows, like in trend analysis)</li>
                    <li><b>Timing</b>: events for a window can appear <i>early</i> or <i>late</i></li>
                </ul>
                <aside class="notes">
                    <p>Fraud detection: sliding window of 1 day</p>
                    <p>Actionable insights: tumbling window of 1 hour</p>
                </aside>
            </section>

            <section>
                <h2>Watermarks</h2>
                <ul>
                    <li><i>Watermarks</i> are timestamps that trigger the computation of the window</li>
                    <li>They are generated at a time that allows a bit of slack for <i>late events</i></li>
<!--                    <li class="fragment" data-fragment-index="1">Any event that reaches the processor later-->
<!--                        than the watermark, but with an event time that should belong to the former window, is <i>ignored</i></li>-->
<!--                    <li class="fragment" data-fragment-index="2">In Flink, it's possible to allow late events to trigger re-computation-->
<!--                        of a window by setting the <i>allowedLateness</i> property</li>-->
                </ul>
            </section>

<!--            <section>-->
<!--                <h2>Consistency semantics</h2>-->
<!--                <ul>-->
<!--                    <li>By configuring offsets, state, and savepointing/checkpointing, a stream processing system can reach a level of robustness:</li>-->
<!--                    <ul>-->
<!--                        <li><b>At-least-once</b>: guarantees that each event is always processed</li>-->
<!--                        <li><b>At-most-once</b>: guarantees that an event is never processed more than once</li>-->
<!--                        <li><b>Exactly-once</b>: guarantees that an event is alyways processed once</li>-->
<!--                    </ul>-->
<!--                </ul>-->
<!--            </section>-->

            <section>
                <h2>Model scoring</h2>
                <ul>
                    <li>To determine the follow-up action of a aggregated business event (e.g. pattern), we have to enrich the event with customer data</li>
                    <li>The resulting data object contains the characteristics (features) as input for a model</li>
<!--                    <li class="fragment" data-fragment-index="1">To get the features and score the model, efficiency plays a role again:</li>-->
<!--                    <ul class="fragment" data-fragment-index="1">-->
<!--                        <li>Direct database call > API call</li>-->
<!--                        <li>In-memory model cache > model on disk</li>-->
<!--                    </ul>-->
                </ul>
            </section>

            <section>
                <h2>PMML</h2>
                <ul>
                    <li>PMML is the glue between data science and data engineering</li>
                    <li>Data scientists can export their machine learning models to PMML (or PFA) format</li>
                </ul>
                <pre><code data-trim>
from sklearn.linear_model import LogisticRegression
from sklearn2pmml import sklearn2pmml

events_df = pandas.read_csv("events.csv")

pipeline = PMMLPipeline(...)
pipeline.fit(events_df, events_df["notifications"])

sklearn2pmml(pipeline, "LogisticRegression.pmml", with_repr = True)
            </code></pre>
            </section>

            <section>
                <h2>PMML</h2>
                <img src="img/PMML.png" class="stretch" />
            </section>

<!--            <section>-->
<!--                <h2>Model scoring</h2>-->
<!--                <ul>-->
<!--                    <li>The models can be loaded into memory to enable split-second performance</li>-->
<!--                    <li>By applying <i>map</i> functions over the events we can process/transform the-->
<!--                        data in the windows:</li>-->
<!--                        <ol>-->
<!--                            <li>enrich each business event by getting more data</li>-->
<!--                            <li>filtering events based on selection criteria (rules)</li>-->
<!--                            <li>score a machine learning model on each event</li>-->
<!--                            <li>write the outcome to a new event / output stream</li>-->
<!--                        </ol>-->
<!--                    </li>-->
<!--                </ul>-->
<!--            </section>-->

            <section>
                <h2>Alternative stacks</h2>
                <small>
                <table>
                    <thead>
                    <tr>
                        <th>Message bus</th>
                        <th>Streaming technology</th>
                        <th>Database</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>Kafka</td>
                        <td>Spark Structured Streaming</td>
                        <td>Ignite</td>
                    </tr>
                    <tr>
                        <td>Kafka</td>
                        <td>Flink</td>
                        <td>Cassandra</td>
                    </tr>
                    <tr>
                        <td>Azure Event Hubs</td>
                        <td>Azure Stream Analytics</td>
                        <td>Cosmos DB</td>
                    </tr>
                    <tr>
                        <td>AWS Kinesis Data Streams</td>
                        <td>AWS Kinesis Data Firehose</td>
                        <td>DynamoDb</td>
                    </tr>

                    </tbody>
                </table>
                </small>
            </section>

            <section>
                <h2>Wrap-up</h2>
                <ul>
                    <li>Fraud detection and actionable insights are examples of streaming analytics use cases in financial services</li>
                    <li>The common pattern is: CEP &rarr; ML &rarr; Notification</li>
                    <li>Pick the right tools for the job; Kafka, Ignite, and Spark are amongst the best</li>
                    <li>Be aware of typical streaming data issues: late events, state management, windows, etc.</li>
                </ul>
            </section>

            <section>
                <h2>Thanks!</h2>
                <small>
                    <p>Read more about streaming analytics at:</p>
                    <ul>
                        <li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf">Google
                            Dataflow paper</a></li>
                        <li><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">The world beyond batch:
                                Streaming 101</a></li>
                        <li><a href="https://online-learning.tudelft.nl/courses/taming-big-data-streams-real-time-data-processing-at-scale/">Real-time Data Processing at Scale (TU Delft)</a></li>
                    </ul>
                    <p>Source code and presentation are available on Github:</p>
                    <p>
                        <a href="https://github.com/streaming-analytics/Styx">https://github.com/streaming-analytics/Styx</a>
                    </p>
                    <p>Please connect at <a href="https://aizonic.com">Aizonic.com</a>, <a href="https://nl.linkedin.com/in/geerdink">LinkedIn</a> or <a href="https://twitter.com/bgeerdink">Twitter</a></p>
                </small>
                <a href="https://aizonic.com"><img src="img/Aizonic.png" style="width: 200px;"></a>
            </section>

        </div>

    </div> <!-- reveal -->

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
        // More info https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            transition: 'slide', // none/fade/slide/convex/concave/zoom

            // More info https://github.com/hakimel/reveal.js#dependencies
            dependencies: [
                { src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
                { src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: 'plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
                { src: 'plugin/search/search.js', async: true },
                { src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/notes/notes.js', async: true }
            ]
        });

    </script>

</body>

</html>